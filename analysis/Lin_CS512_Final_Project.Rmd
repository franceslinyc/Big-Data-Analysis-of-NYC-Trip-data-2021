---
title: "Big Data Analysis of NYC TLC Trip Record Data"
author: "Frances Lin"
date: "March 2021"
output: pdf_document
---

# Background and Objective

My answer to the question "Where would you like to visit next?" is always Iceland. This is true even prior to COVID-19. While we are all stuck at home 24/7, I thought it might be nice to at least send my mind off to somewhere magical. However, after long hours search with minimal luck, I "settled" for this still quite interesting and fairly well-known, large (>100 GB) open dataset: NYC TLC trip record data. 

The objective of this project is to answer the following questions of interest, which include 

1. How does monthly average fare amount vary by year (2019 vs 2020) and by taxi type (yellow taxi vs green taxi)? 

2. How does total count of high volume fhv (for-hire vehicle) services vary by year (2019 vs 2020) and by license type (Uber, Lyft, Via, Juno)? 

3. What are the top 6 drop-off locations and how do they vary by year (2019 vs 2020) and by vehicle type (yellow taxi, green taxi, fhv, hfhv)? 




# 1. Obtain

## Overview of Dataset

TLC trip record data is obtained from the New York City Taxi & Limousine Commission's site: [*link*](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Overall, the data is split across multiple **csv** files. It is composed of more than one table of related data. The data is accessed through creating multiple **txt** files that contain list of **url**s and using the `wget` commend  in VM. The entire dataset is 128.7 GB in total. (All the files were uploaded to **Google** **Storage** but only a subset of it were used, which is 35.2662 GB in total.) \ 

**Data Description** 

The yellow and green taxi and for-hire vehicle (fhv) trip records contain records from Jan 2019 to Dec 2020. The high volume for-hire vehicle (hfhv) trip records contain records from Feb 2019 to Dec 2020. *(Note that on August 14, 2018, a Local Law of 2018 was signed to create a new license category for TLC-licensed businesses that currently or plan to dispatch more than 10,0000 for-hire vehicle (fhv) trips in NYC per day. These businesses include Juno, Uber, Via, and Lyft.)* \ 

Each record contains trips within a month. Individual row in each record represents individual trip. 

The taxi+ zone lookup record contains location information such as `Borough` (e.g. Brooklyn, Manhattan) and `Zone` (e.g. Crown Heights North, Upper East Side North). Corresponding maps in **jpg**s can be found on the same site that contains these trip records. \ 




**Fields Description** 

The yellow and green taxi trip records contain fields such as `VendorID` (`1=Creative Mobile Technologies, LLC` or `2=VeriFone Inc.`), `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `PULocationID`, `DOLocationID`, `Trip_distance`, `Payment_type`, `Fare_amount`, `Tip_amount`, `Total_amount`, etc. The green taxi trip records has additional field `Trip_type` that is used to indicate whether it is `1=Stree-hail` or `2=Dispatch`.

The fhv and hfhv trip records contain fields such as `Dispatching_base_num`, `Pickup_datetime`, `DropOff_datetime`, `PULocationID`, `DOLocationID`, and `SR_Flag` (`1=shared ride` or `null`). The hfhv trip records has additional field `Hvfhs_license_num` that is used to identify businesses: Juno, Uber, Via, and Lyft.

The taxi zone lookup record contains fields such as `LocationID`, `Borough`, `Zone`, and `service_zone`. `LocationID` can be used to identify `PULocationID` (i.e. pick-up location) and `DOLocationID` (i.e. drop-off location) in all other records. \ 




**Sample of Initial Data** 

a sample of *Yellow Taxi Trip Records 2020-01*: 
<p>
![a sample of *Yellow Taxi Trip Records 2020-01*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/initial_1.png) 
</p>

a sample of *Green Taxi Trip Records 2020-01*:
<p>
![a sample of *Green Taxi Trip Records 2020-01* ](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/initial_2.png)
</p>

a sample of *For-Hire Vehicle Trip Records 2020-01*:
<p>
![a sample of *For-Hire Vehicle Trip Records 2020-01*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/initial_3.png)
</p>

a sample of *High Volume For-Hire Vehicle Trip Records 2020-01*:
<p>
![a sample of *High Volume For-Hire Vehicle Trip Records 2020-01*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/initial_4.png) 
</p>

a sample of *Taxi Zone Lookup Record*: 
<p>
![a sample of *Taxi Zone Lookup Record*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/initial_5.png) 
</p>




Initially, I attempted to make **API** request since the data is host on **Amazon Web Services** and can be copied to **S3** bucket. To save time for processing in **Google** **BigQuery** and **Apache** **Spark**, I directly downloaded the **csv** files from the site to an external hard drive and compressed them into multiple **zip** files instead. The total of the 4 uncompressed file is approx. 132.68 GB, which is compressed to approx. 26.3 GB. \ 

In retrospect, it makes more sense to do as follows: To load data to **BigQuery**, I revisted Week 6's Big Data Module. First, in **Compute Engine**, I created a VM instance with 200 GB and logged in remotely via SSH. Next, I tried to connect SSH to the external hard drive where I saved the 4 **zip** files but did not succeed. Instead, I created a **txt** that contains a list of **url**s and used the `wget` commend to load the data from the site to a bucket in **Storage**. \ 

Inside the bucket, I repeated the process and created separate folders so that trip records of **yellow taxi** (`yellow_data`: *72 objects, 63.8 GB*), **green taxi** (`green_data`: *72 files, 35.4 GB*), **fhv** (`fhv_data`: *72 files, 35.4 GB*) and **hfhv** (`hfhv_data`: *23 files, 22.3 GB*) were stored separately. 

I had trouble previewing the entire dataset in **Dataprep** (I thought they did not load entirely), so I repeated the above process. But this time, instead of separating records by type, I separated records further by year. The combined dataset (35.2662 GB) breaks down to: 

record of 

- `yellow_data_2020`: *12 files, 2.1 GB*, `yellow_data_2019`: *12 files, 7.3 GB*

- `green_data_2020`: *12 files, 150 MB*, `green_data_2019`: *12 files, 528 MB*

- `fhv_data_2020`: *12 files, 688.2 MB*, `fhv_data_2019`: *12 files, 2.2 GB*

- `hfhv_data_2020`: *12 files, 8.4 GB*, `hfhv_data_2019`: *12 files, 13.9 GB*

**snippet of Linux command to load data to GCS (Google Cloud Storage)**

```{}
# Create a directory and set it to current directory
# Reference: https://canvas.oregonstate.edu/courses/1799395/assignments/8130281?module_item_id=20176237
mkdir yellow_data_2020
cd yellow_data_2020

# Create a txt file that contains a list of urls
cat > url_yellow_2020.txt

# Download multiple urls
# Reference: https://shapeshed.com/unix-wget/#how-to-download-multiple-urls
wget -i url_yellow_2020.txt

# Copy to bucket > yellow_data: 72 objects, 63.8 GB
mv url_yellow_2020.txt ../
cd ..
gsutil -m cp -r yellow_data_2020/ gs://cs512_trip
```

buket details in Storage (~ 128.7 GB):
<p>
![buket details in Storage](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/storage.png)
</p>

buket details in Storage - Updated (~ 35.2662 GB):
<p>
![buket details in Storage](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/storage_1.png)
</p>




# 2. Scrub

## Overview of Data Wrangling

I realize now that data wrangling can be an exceedingly long and still ongoing process. Overall, data were wrangled in **Dataprep**, **BigQuery** and **R**, and the process continues in **Spark**. 




**To BigQuery**

Once a dataset is created in **BigQuery**, using **Dataprep**, I attempted to transfer the data from **GCS** to **BigQuery**. The job for yellow taxi records alone took more than 2 hours and still did not complete. Since it seemed that not all data were loaded to **Dataprep** successfully (they were!), as mentioned, I repeated all of the above process: I separated trip records not only by type (i.e. yellow, green taxi, fhv, hfhv) but also by year (i.e. 2015, 2016, and on) and reloaded them to **Storage**.

It makes more sense to proceed with this route (by type & by years) as the final route, although it seems that the previous route (by type only) and then querying by `TIMESTAMP` is doable too. *(2015's dataset, for example, contains features such as* `longitude` *and* `latitude` *that were represented as Taxi Zone in 2019's and 2020's dataset, which further confirms that it may be best to also separate the records by year.)*




**Data wrangling in Dataprep** 

In **Dataprep**, I created a recipe for yellow trip records (and an equivalent recipe was copied and changed imput to green trip records.) Basic steps are as followed: 

1. Delete columns not of interest

2. Delete rows with missing values in `trip_distance` & `total_amount`

3. Delete rows where values <=0 for both `trip_distance` & `total_amount`

4. Extract time from both `tpep_pickup_datetime` & `tpep_dropoff_datetime` (Could be done later in **Dataproc** too.)

5. Delete columns not of interest




recipe of *Yellow Taxi Trip Records 2020-01 to 2020-12*:
<p>
![recipe of *Yellow Taxi Trip Records*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/receipt_yellow.png){width=30%, height=30%}
</p>

recipe of *Green Taxi Trip Records 2020-01 to 2020-12*:
<p>
![recipe of *Green Taxi Trip Records*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/receipt_green.png){width=30%, height=30%} 
</p>




I created a recipe for fhv trip records (and an equivalent recipe was copied and changed imput to hfhv trip records.) Basic steps are as followed: 

1. Extract time from both `pickup_datetime` & `dropoff_datetime` (Again, could be done later in **Dataproc** too.)

2. Delete rows with missing values in `pickup_time` & `dropoff_time`

3. Delete rows where values <=0 for both `trip_distance` & `total_amount`

For hfhv trip records only: 

4. Add a column named `license` indicating whether it is ran by Juno, Uber, Via, or Lyft

recipe of *For-Hire Vehicle Trip Records 2020-01 to 2020-12*:
<p>
![recipe of *For-Hire Vehicle Trip Records*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/receipt_fhv.png){width=30%, height=30%}  
</p>

recipe of *High Volume For-Hire Vehicle Trip Records 2020-01 to 2020-12*: 
<p>
![recipe of *High Volume For-Hire Vehicle Trip Records*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/receipt_hfhv.png){width=30%, height=30%}  
</p>




**Preview screens in BigQuery** 

preview of *Yellow & Green Taxi Trip Records 2020-01 to 2020-12*:
<p>
![preview of *Yellow Taxi Trip Records*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/bigquery_yellow.png)
</p

preview of *Green Taxi Trip Records 2020-01 to 2020-12*:
<p>
![preview of *Green Taxi Trip Records*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/bigquery_green.png) 
</p>

preview of *For-Hire Vehicle Trip Records 2020-01 to 2020-12*:
<p>
![preview of *For-Hire Vehicle Trip Records*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/bigquery_fhv.png) 
</p>

preview of *High Volume For-Hire Vehicle Trip Records 2020-01 to 2020-12*:
<p>
![preview of *High Volume For-Hire Vehicle Trip Records*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/bigquery_hfhv.png) 
</p>




**To PySpark**

To load data to **Spark**, I revisted Week 7's Spark Module and followed part of the starter code provided for the Plane Distance assignment. Once the **py** file is created, I uploaded this file to **Storage**. Then, I created a cluster and submitted a job in **Dataproc**. \ 

**test snippet of Python script to run Dataproc job**

```{}
# Connect to Spark
sc = pyspark.SparkContext() # run Spark applications
#PACKAGE_EXTENSIONS= ('gs://hadoop-lib/bigquery/bigquery-connector-hadoop2-latest.jar')

bucket = sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')
project = sc._jsc.hadoopConfiguration().get('fs.gs.project.id')
input_directory = 'gs://{}/hadoop/tmp/bigquerry/pyspark_input'.format(bucket)
output_directory = 'gs://{}/pyspark_demo_output'.format(bucket)

conf={
    # change project id, dataset id, table id
    'mapred.bq.project.id':project,
    'mapred.bq.gcs.bucket':bucket,
    'mapred.bq.temp.gcs.path':input_directory,
    'mapred.bq.input.project.id': "ultra-dimension-300900", 
    'mapred.bq.input.dataset.id': 'nyctrip_data', 
    'mapred.bq.input.table.id': 'yellow_data_2020_20210309_002840', 
}
```

```{}
# Pull table from big query
table_data = sc.newAPIHadoopRDD(
    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',
    'org.apache.hadoop.io.LongWritable',
    'com.google.gson.JsonObject',
    conf = conf)
```

```{}
# Convert table to a json like object
vals = table_data.values()
vals = vals.map(lambda line: json.loads(line))
pprint.pprint(vals.first())
```


**test job output of *Yellow Taxi Trip Records 2020-01 to 2020-12**
<p>
![test job of *Yellow Taxi Trip Records 2020-01 to 2020-12*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/job_to_spark.png) 
</p>


For now, I just want to show that the table loaded successfully as a **JSON** object. We will come back to **Spark** later in the **4. Model** section. 




# 3. Explore (and More Scrub) 

A lot of the exploration were done in the previous **2. Scrub** section. Further exploration were done going back to **Dataprep**. Indeed, it seemed like it is much easier to extract `datetime` and `time` further to `year`, `month`, `day`, `hour`, `minute`, and `sec` in **Dataprep**. I went back to pull `datetime` out for all the records. \ 




preview of *Yellow Taxi Trip Records 2020-01 to 2020-12* - Updated:
<p>
![preview of *Yellow Taxi Trip Records*](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/bigquery_yellow_updated.png)
</p>




Results in this section were mostly done through querying in **BigQuery** first. I attempted to join tables using a wildcard table: [*link*](https://cloud.google.com/bigquery/docs/querying-wildcard-tables). However, because of some of the syntax differences between **BigQuery** and standard **SQL**, I did not quite succeed and will come back to this if time permits. Since the resulting dataset is relatively small and to save time, I downloaded query results to my local drive and further manipulated (e.g. reformatting, joining) and plotted the data using the `tidyverse`, `dplyr`, and `ggplot2` package of **R** in local drive instead. \ 


## Data Analysis Questions 

For the data analysis questions, I am interested in: 

### Question 1: How does monthly average fare amount vary by year (2019 vs 2020) and by taxi type (yellow taxi vs green taxi)? 

To answer this question, I used the `AVG` and `GROUP BY` function in **BigQuery** to compute average fare amount by month. Then, I downloaded query results for all records and loaded them in **R**. I further reformatted and combined the dataframes from there since the `ggplot` package of R requires table format to be in the long format (This can be done using the `tidyr` package too.) Then, I made some scatterplots and overlaid smooth lines. \ 

Results show that while it appears that monthly average fare amount for yellow taxi shows a decreasing trend and that for green taxi shows an increasing one, the differences may not be significant (Max difference is \$$9.116099$, which occurs at green taxi trip records.) I am also unsure why there seems to an increase of monthly average fare amount for 2020 green taxi but information could often be lost through aggregation. I also think that perhaps median could do a better job than mean does. Further wrangling and/or analysis could look into that. \ 

Figure 1. 
<p>
![p_2](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/p_2.png)
</p>

**snippet of SQL script to query data in BigQuery**

```{}
# Compute monthly avg of total_amount
SELECT 
pickup_mon, 
AVG(total_amount) AS mon_amount, 
FROM trip_data.yellow_data_2019
GROUP BY pickup_mon
ORDER BY pickup_mon ASC;
```

**snippet of R script to manipulate and plot data can be found in Lin_Plotting.pdf (or Lin_Plotting.Rmd)**




### Question 2: How does total count of high volume fhv (for-hire vehicle) services vary by year (2019 vs 2020) and by license type (Uber, Lyft, Via, Juno)? 

To answer this question, I followed similar steps: I counted yearly total count of dispatch by license type for high volume fhv. Recall that to be considered as hfhv, businesses either currently dispatch or plan to dispatch more than 10,000 fhv trips per day in NYC. Note also that Juno is no longer considered as hfhv as of 2020, but I included them in the resulting dataframe for plotting purposes. \ 

Results show that Uber dispatched the most fhv trips overall and Lyft dispatched the second most trips from 2019-2020. And, not to my surprise is that while the number of trips for all businesses still remain high (>10,000 trips/day in NYC), it appears that all businesses have been affected during the the COVID-19 disruption. The number of trips are consistently lower and as mentioned, Juno is no longer considered as hfhv as of 2020. Further wrangling and/or analysis could look into exactly which month did the drop in trip occur. My guess is that it occured in early 2020 when COVID-19 restrictions were in effect in NYC. \ 




Figure 2. 
<p>
![p_license](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/p_license.png)
</p>

**snippet of SQL script to query data in BigQuery**

```{}
# Compute count yearly license
SELECT 
license, 
COUNT(license) AS count_license
FROM trip_data.hfhv_data_2020
GROUP BY license
ORDER BY count_license DESC;
```

**snippet of R script to manipulate and plot data can be found in Lin_Plotting.pdf (or Lin_Plotting.Rmd)**




### Question 3: What are the top 6 drop-off locations and how do they vary by year (2019 vs 2020) and by vehicle type (yellow taxi, green taxi, fhv, hfhv)? 

To answer this question, I counted yearly total count of drop-off location ID, ordered them and selected top 6 count of drop-off location ID in **BigQuery** for all vehicle types. Once I have query results for all vehicle types, I joined these tables along with the taxi+ zone lookup table and using the `full_join` function in the `dplyr` package of **R** and location ID as key. \ Then, I plotted the count of top 6 drop-off location in term of `Borough` and `Zone` by year & by vehicle type.

Results show that Manhattan is top drop-off location for yellow taxi, and Queens, Brooklyn, and Manhattan are top drop-off locations for hfhv. There are a lot of unknown drop-off location for fhv (<10,000 trips/day in NYC). This is because in the taxi+ zone table `ID=264` corresponds to `Unknown Borough` and `NV Zone` and `ID=265` corresponds to `Unknown Borough` and `null Zone`. 

Comparing the records across 2019 and 2020, we also see that similar trend as a result of COVID-19 disruption can be observed here. I get curious about specific zone of the drop-off location, so I also include a plot of top drop-off locations further broken down by zone, which is shown in Figure 4. 


\ 




Figure 3. 
<p>
![p_Borough](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/p_Borough.png)
</p>

Figure 4.
<p>
![p_Zone](/Users/franceslinyc/Big-Data-Analysis-of-NYC-Trip-data-2021/p_Zone.png)
</p>

**snippet of SQL script to query data in BigQuery**

```{}
# Compute top 6 count of DOLocationID 
SELECT
DISTINCT DOLocationID, 
COUNT(DOLocationID) as DOLocationID_count
FROM trip_data.hfhv_data_2020
GROUP BY DOLocationID
ORDER BY DOLocationID_count DESC
LIMIT 6; 
```

**snippet of R script to manipulate and plot data can be found in Lin_Plotting.pdf (or Lin_Plotting.Rmd)**




# 4. Model

A lot of the modeling were done in the previous **3. Explore** section. However, during this stage, I notice that there are additional wrangling that needs to be done. Given how much time I have left for this project and the remaining days of **Google Cloud**'s free trial, I am just gonna do as much as I could and leave the rest for future analysis. \ 




**To PySpark - Updated**

To load and model data in **Spark**, I revisted Week 7's Spark and Week 8's Spark Specifics Module. 

### Bonus Question 1: How does monthly average fare amount vary for 2020 yellow taxi records? 

I have to admit that spending 1-2 weeks on **Spark** prior to completing this project is NOT enough. Instead of going back to work on additional wrangling in **Dataprep**, I decide to spend the remaining 2 days working in **Spark** to get additional practice for it, even if it means I will lose points for accuracy for this project. 

Average trip distance (in meter) and average fare amount (in $) were computed loading the table from **BigQuery** and converting it to a **PySpark** dataframe first. Then, I computed mean, standard deviation and count of trip distance and fare amount by month using the `mean`, `stddev` and `count` along with the `groupBy` and `agg` function from the **pyspark.sql** module.  

Results show that May has the longest mean trip distance (8.6103 m) and June has the highest mean fare amount ($18.7660). However, since the standard deviation is considerably high for both `trip_distance` and `total_amount` for several months, further wrangling should consider creating an IQR (interquartile range) function (e.g. `map(lambda x: )`) or using a build-in function to identify outliers and further filter them out. 

```{}
+----------+------------------+--------------------------+--------------------+
|pickup_mon|avg(trip_distance)|stddev_samp(trip_distance)|count(trip_distance)|
+----------+------------------+--------------------------+--------------------+
|         7|4.4518635390300245|        482.28774289655644|              772188|
|        11| 4.205144641983849|         445.6264180067282|             1483188|
|         3|3.1527776105306087|         208.3966193939616|             2965225|
|         8|  4.51186787270762|         339.0286619643721|              975016|
|         5| 8.610368613222507|         788.0716701210021|              336793|
|         9| 4.365925615783489|         446.6348380792307|             1312695|
|         6| 4.246858455393626|          342.203886469089|              530081|
|         1| 2.968089891868472|         83.73389022813812|             6317192|
|        10| 3.576072340784534|         282.6028806567841|             1650166|
|        12| 6.163896306510167|         894.0040816367627|             1437551|
|         4| 4.157984192049327|        299.66132677636926|              230456|
|         2|2.8946657780171594|        40.527459490047455|             6220575|
+----------+------------------+--------------------------+--------------------+

+----------+------------------+-------------------------+-------------------+
|pickup_mon| avg(total_amount)|stddev_samp(total_amount)|count(total_amount)|
+----------+------------------+-------------------------+-------------------+
|         7|18.585729792231376|        14.33372914300685|             772188|
|        11| 17.55121064270372|       124.93683514431557|            1483188|
|         3|18.501154053175494|        389.9437664659958|            2965225|
|         8|18.572788916927294|        190.3048652763212|             975016|
|         5| 18.45184195321015|        14.79325219081066|             336793|
|         9|17.680980998195682|        11.85518455438868|            1312695|
|         6|18.766022097942624|       14.672790280717793|             530081|
|         1|18.600994187967427|       14.101483341856811|            6317192|
|        10|18.138897976417812|        777.2273359813906|            1650166|
|        12|17.435560898038982|       13.612546930941804|            1437551|
|         4|16.421087725317346|       12.463365276397795|             230456|
|         2|18.558698606989672|       13.853541917402083|            6220575|
+----------+------------------+-------------------------+-------------------+
```

**snippet of Python script and job ouput can be found in the Appendix section of this .pdf** 




### Bonus Question 2: What is the yearly average fare amount and avg distance for 2020 yellow taxi records? 

It has become very clear that we will either have to do further wrangling here in **Dataproc** for **PySpark** or go back to **Dataprep** for it since ensuring these values are non-negative in **Dataprep** is not enough. The output is shown as follows:  

```{}
+-------+------------------+
|summary|     trip_distance|
+-------+------------------+
|  count|          24231126|
|   mean|3.5814057339315424|
| stddev| 327.8228554760617|
|    min|              0.01|
|    max|         350914.88|
+-------+------------------+

+-------+------------------+
|summary|      total_amount|
+-------+------------------+
|  count|          24231126|
|   mean|18.342393123230288|
| stddev|249.58870028047656|
|    min|               0.3|
|    max|          998325.6|
+-------+------------------+
```

**snippet of Python script and job ouput can be found in the Appendix section of this .pdf** 




# 5. Interpreting (and Discussing)

Results can be found in the **3. Explore** section, but to sum up, it appeared that the total amount of trips have reduced since the COVID-19 disruption. Specifically, monthly average fare amount for yellow taxi shows a decreasing trend from 2019-2020. In addition, the number of trips from 2020 for all hfhv businesses are consistently lower than that from 2019. 

Results are not error free, but I am okay with what I have at this point. After all, the purpose of EDA (e.g. plot, summary statistics) is to ensure that the values are accurate and questions are not only interesting but also potentially statistically significant prior to doing any modeling, which I unfortunately ran out of time for. The following is what I will do in **Spark** next if time permits: 

1. Create an IQR function to identify outliers 

2. Remove the outliers

3. Re-compute summary statistics, etc

Finally, I plan to sign up CS513 Applied Machine Learning and use this dataset for further wrangling and analysis, but my **Google** **Cloud** free trial is few days away from expiring. I also just get accepted to the PhD in statistics program here at OSU, so I am going to take my time to take CS534 Machine Learning instead — what a challenging yet rewarding class it is, all I want to say! 

\newpage

# Appendix

**snippet of Python script to run Dataproc job** **— Bonus Question 1**

```{}
import pyspark
from pyspark.sql import SparkSession
import pprint
import json
from pyspark.sql.types import StructField, StructType 
from pyspark.sql.types import StringType, FloatType, IntegerType, DateType, TimestampType
from pyspark.sql import functions

# Connect to Spark
sc = pyspark.SparkContext() # run Spark applications
#PACKAGE_EXTENSIONS= ('gs://hadoop-lib/bigquery/bigquery-connector-hadoop2-latest.jar')

bucket = sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')
project = sc._jsc.hadoopConfiguration().get('fs.gs.project.id')
input_directory = 'gs://{}/hadoop/tmp/bigquerry/pyspark_input'.format(bucket)
output_directory = 'gs://{}/pyspark_demo_output'.format(bucket)

conf={
    # change project id, dataset id, table id
    'mapred.bq.project.id':project,
    'mapred.bq.gcs.bucket':bucket,
    'mapred.bq.temp.gcs.path':input_directory,
    'mapred.bq.input.project.id': "ultra-dimension-300900", 
    'mapred.bq.input.dataset.id': 'trip_data', 
    'mapred.bq.input.table.id': 'yellow_data_2020_20210310_070223', 
}

# Pull table from big query
table_data = sc.newAPIHadoopRDD(
    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',
    'org.apache.hadoop.io.LongWritable',
    'com.google.gson.JsonObject',
    conf = conf)

# Convert table to a json like object
vals = table_data.values()
vals = vals.map(lambda line: json.loads(line))
#pprint.pprint(vals.first()) # good as of 03/13/2021

# Define a To_numb function 
def To_numb(x):
  x['pickup_hr'] = int(x['pickup_hr'])
  x['pickup_day'] = int(x['pickup_day'])

  x['pickup_yr'] = int(x['pickup_yr'])
  x['dropoff_hr'] = int(x['pickup_hr'])
  x['dropoff_day'] = int(x['pickup_day'])
  x['dropoff_mon'] = int(x['pickup_mon'])
  x['dropoff_yr'] = int(x['pickup_yr'])
  x['trip_distance'] = float(x['trip_distance'])
  x['PULocationID'] = int(x['PULocationID'])
  x['DOLocationID'] = int(x['DOLocationID'])
  x['total_amount'] = float(x['total_amount'])
  return x

# Apply To_numb function to int or float variables otherwise schema won't work 
vals = vals.map(To_numb)

# Create a dataframe object

# schema
# https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.html
schema = StructType([
   StructField('tpep_pickup_datetime', StringType(), True),  # TimestampType() Not in the form?
   StructField("pickup_hr", IntegerType(), True), 
   StructField("pickup_day", IntegerType(), True),
   StructField("pickup_mon", StringType(), True),     # change back to StringType()
   StructField("pickup_yr", IntegerType(), True),     
   StructField("tpep_dropoff_datetime", StringType(), True), # TimestampType() Not in the form?
   StructField("dropoff_hr", IntegerType(), True), 
   StructField("dropoff_day", IntegerType(), True),
   StructField("dropoff_mon", IntegerType(), True),      
   StructField("dropoff_yr", IntegerType(), True),
   StructField("trip_distance", FloatType(), True),      
   StructField("PULocationID", IntegerType(), True), 
   StructField("DOLocationID", IntegerType(), True),
   StructField("total_amount", FloatType(), True)])

#pprint.pprint(vals.first()) # good as of 03/13/2021

# Initialize spark
# https://spark.apache.org/docs/2.0.0/sql-programming-guide.html#sql
spark = SparkSession\
    .builder\
    .appName("PythonSQL")\
    .getOrCreate()

# Create a df 
df1 = spark.createDataFrame(vals, schema= schema)
df1.repartition(6)                  # partition to 6 partitions # could partition by key too

#pprint.pprint(vals.first())        # good as of 03/09/2020

# Need a To_numb function as well 


# Compute summary statistics
#df1.describe("trip_distance").show() # good
#df1.describe(["trip_distance", "total_amount"]).show() # does not work

#df1.describe("trip_distance").show()
#df1.describe("total_amount").show()

# Query data
# https://towardsdatascience.com/beginners-guide-to-pyspark-bbe3b553b79f

# Compute monthly avg trip_distance & total_amount
# df1.select('trip_distance'
#           ).groupBy('pickup_mon')\
#           .mean()\
#           .show()
#AttributeError: 'GroupedData' object has no attribute 'describe'

# Try this instead
# https://stackoverflow.com/questions/51632126/pysparkhow-to-calculate-avg-and-count-in-a-single-groupby
df1.groupBy('pickup_mon').agg(functions.mean('trip_distance'), functions.stddev('trip_distance'), functions.count('trip_distance')).show()
df1.groupBy('pickup_mon').agg(functions.mean('total_amount'), functions.stddev('total_amount'), functions.count('total_amount')).show()

# Compute stddev next
# Could probably compute IQR from functions or from the stddeve from above too

# Consider map(lambda x:) for individual variable and compute summary statistics; it may be faster?




# Delete the temporary files
input_path = sc._jvm.org.apache.hadoop.fs.Path(input_directory)
input_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(input_path, True) 

## Back to Google Cloud, Week 7
## Upload this file to Storage's cs512_trip
## Create cluster and submit job in Dataproc
## Copy & paste this to Jar files
## gs://hadoop-lib/bigquery/bigquery-connector-hadoop2-latest.jar
```

**Job output** **— Bonus Question 1**

```{}
21/03/15 23:21:58 INFO org.sparkproject.jetty.util.log: Logging initialized @4408ms to org.sparkproject.jetty.util.log.Slf4jLog
21/03/15 23:21:58 INFO org.sparkproject.jetty.server.Server: jetty-9.4.36.v20210114; built: 2021-01-14T16:44:28.689Z; git: 238ec6997c7806b055319a6d11f8ae7564adc0de; jvm 1.8.0_282-b08
21/03/15 23:21:58 INFO org.sparkproject.jetty.server.Server: Started @4528ms
21/03/15 23:21:58 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@1cc6b5d9{HTTP/1.1, (http/1.1)}{0.0.0.0:38759}
21/03/15 23:21:59 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-66dc-m/10.138.0.28:8032
21/03/15 23:21:59 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-66dc-m/10.138.0.28:10200
21/03/15 23:21:59 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
21/03/15 23:21:59 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
21/03/15 23:22:02 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1615850475068_0001
21/03/15 23:22:03 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-66dc-m/10.138.0.28:8030
21/03/15 23:22:05 INFO com.google.cloud.hadoop.io.bigquery.BigQueryFactory: BigQuery connector version hadoop2-1.2.0
21/03/15 23:22:05 INFO com.google.cloud.hadoop.io.bigquery.BigQueryFactory: Creating BigQuery from default credential.
21/03/15 23:22:05 INFO com.google.cloud.hadoop.io.bigquery.BigQueryFactory: Creating BigQuery from given credential.
21/03/15 23:22:05 INFO com.google.cloud.hadoop.io.bigquery.BigQueryConfiguration: Using working path: 'gs://dataproc-staging-us-west1-283579064512-mj5qi01m/hadoop/tmp/bigquerry/pyspark_input'
21/03/15 23:25:51 INFO com.google.cloud.hadoop.io.bigquery.UnshardedExportToCloudStorage: Setting FileInputFormat's inputPath to 'gs://dataproc-staging-us-west1-283579064512-mj5qi01m/hadoop/tmp/bigquerry/pyspark_input'
21/03/15 23:25:51 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 6
+----------+------------------+--------------------------+--------------------+
|pickup_mon|avg(trip_distance)|stddev_samp(trip_distance)|count(trip_distance)|
+----------+------------------+--------------------------+--------------------+
|         7|4.4518635390300245|        482.28774289655644|              772188|
|        11| 4.205144641983849|         445.6264180067282|             1483188|
|         3|3.1527776105306087|         208.3966193939616|             2965225|
|         8|  4.51186787270762|         339.0286619643721|              975016|
|         5| 8.610368613222507|         788.0716701210021|              336793|
|         9| 4.365925615783489|         446.6348380792307|             1312695|
|         6| 4.246858455393626|          342.203886469089|              530081|
|         1| 2.968089891868472|         83.73389022813812|             6317192|
|        10| 3.576072340784534|         282.6028806567841|             1650166|
|        12| 6.163896306510167|         894.0040816367627|             1437551|
|         4| 4.157984192049327|        299.66132677636926|              230456|
|         2|2.8946657780171594|        40.527459490047455|             6220575|
+----------+------------------+--------------------------+--------------------+

+----------+------------------+-------------------------+-------------------+
|pickup_mon| avg(total_amount)|stddev_samp(total_amount)|count(total_amount)|
+----------+------------------+-------------------------+-------------------+
|         7|18.585729792231376|        14.33372914300685|             772188|
|        11| 17.55121064270372|       124.93683514431557|            1483188|
|         3|18.501154053175494|        389.9437664659958|            2965225|
|         8|18.572788916927294|        190.3048652763212|             975016|
|         5| 18.45184195321015|        14.79325219081066|             336793|
|         9|17.680980998195682|        11.85518455438868|            1312695|
|         6|18.766022097942624|       14.672790280717793|             530081|
|         1|18.600994187967427|       14.101483341856811|            6317192|
|        10|18.138897976417812|        777.2273359813906|            1650166|
|        12|17.435560898038982|       13.612546930941804|            1437551|
|         4|16.421087725317346|       12.463365276397795|             230456|
|         2|18.558698606989672|       13.853541917402083|            6220575|
+----------+------------------+-------------------------+-------------------+

21/03/15 23:31:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-staging-us-west1-283579064512-mj5qi01m/hadoop/tmp/bigquerry/' directory.
21/03/15 23:31:18 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@1cc6b5d9{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
Job output is complete
```

**snippet of Python script to run Dataproc job** **— Bonus Question 2**

```{}
import pyspark
from pyspark.sql import SparkSession
import pprint
import json
from pyspark.sql.types import StructField, StructType 
from pyspark.sql.types import StringType, FloatType, IntegerType, DateType, TimestampType

# Connect to Spark
sc = pyspark.SparkContext() # run Spark applications
#PACKAGE_EXTENSIONS= ('gs://hadoop-lib/bigquery/bigquery-connector-hadoop2-latest.jar')

bucket = sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')
project = sc._jsc.hadoopConfiguration().get('fs.gs.project.id')
input_directory = 'gs://{}/hadoop/tmp/bigquerry/pyspark_input'.format(bucket)
output_directory = 'gs://{}/pyspark_demo_output'.format(bucket)

conf={
    # change project id, dataset id, table id
    'mapred.bq.project.id':project,
    'mapred.bq.gcs.bucket':bucket,
    'mapred.bq.temp.gcs.path':input_directory,
    'mapred.bq.input.project.id': "ultra-dimension-300900", 
    'mapred.bq.input.dataset.id': 'trip_data', 
    'mapred.bq.input.table.id': 'yellow_data_2020_20210310_070223', 
}

# Pull table from big query
table_data = sc.newAPIHadoopRDD(
    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',
    'org.apache.hadoop.io.LongWritable',
    'com.google.gson.JsonObject',
    conf = conf)

# Convert table to a json like object
vals = table_data.values()
vals = vals.map(lambda line: json.loads(line))
#pprint.pprint(vals.first()) # good as of 03/13/2021

# Define a To_numb function 
def To_numb(x):
  x['pickup_hr'] = int(x['pickup_hr'])
  x['pickup_day'] = int(x['pickup_day'])
  x['pickup_mon'] = int(x['pickup_mon'])
  x['pickup_yr'] = int(x['pickup_yr'])
  x['dropoff_hr'] = int(x['pickup_hr'])
  x['dropoff_day'] = int(x['pickup_day'])
  x['dropoff_mon'] = int(x['pickup_mon'])
  x['dropoff_yr'] = int(x['pickup_yr'])
  x['trip_distance'] = float(x['trip_distance'])
  x['PULocationID'] = int(x['PULocationID'])
  x['DOLocationID'] = int(x['DOLocationID'])
  x['total_amount'] = float(x['total_amount'])
  return x

# Apply To_numb function to int or float variables otherwise schema won't work 
vals = vals.map(To_numb)

# Create a dataframe object

# schema
# https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.html
schema = StructType([
   StructField('tpep_pickup_datetime', StringType(), True),  # TimestampType() Not in the form?
   StructField("pickup_hr", IntegerType(), True), 
   StructField("pickup_day", IntegerType(), True),
   StructField("pickup_mon", IntegerType(), True),     
   StructField("pickup_yr", IntegerType(), True),     
   StructField("tpep_dropoff_datetime", StringType(), True), # TimestampType() Not in the form?
   StructField("dropoff_hr", IntegerType(), True), 
   StructField("dropoff_day", IntegerType(), True),
   StructField("dropoff_mon", IntegerType(), True),      
   StructField("dropoff_yr", IntegerType(), True),
   StructField("trip_distance", FloatType(), True),      
   StructField("PULocationID", IntegerType(), True), 
   StructField("DOLocationID", IntegerType(), True),
   StructField("total_amount", FloatType(), True)])

#pprint.pprint(vals.first()) # good as of 03/13/2021

# Initialize spark
# https://spark.apache.org/docs/2.0.0/sql-programming-guide.html#sql
spark = SparkSession\
    .builder\
    .appName("PythonSQL")\
    .getOrCreate()

# Create a df 
df1 = spark.createDataFrame(vals, schema= schema)
df1.repartition(6)                  # partition to 6 partitions 

#pprint.pprint(vals.first())        # good as of 03/09/2020

# Need a To_numb function as well 


# Compute summary statistics
#df1.describe("trip_distance").show() # good
#df1.describe(["trip_distance", "total_amount"]).show() # does not work

df1.describe("trip_distance").show()
df1.describe("total_amount").show()

# Delete the temporary files
input_path = sc._jvm.org.apache.hadoop.fs.Path(input_directory)
input_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(input_path, True) 

## Back to Google Cloud, Week 7
## Upload this file to Storage's cs512_trip
## Create cluster and submit job in Dataproc
## Copy & paste this to Jar files
## gs://hadoop-lib/bigquery/bigquery-connector-hadoop2-latest.jar
```

**Job output** **— Bonus Question 2**

```{}
21/03/14 03:10:05 INFO org.sparkproject.jetty.util.log: Logging initialized @4957ms to org.sparkproject.jetty.util.log.Slf4jLog
21/03/14 03:10:06 INFO org.sparkproject.jetty.server.Server: jetty-9.4.36.v20210114; built: 2021-01-14T16:44:28.689Z; git: 238ec6997c7806b055319a6d11f8ae7564adc0de; jvm 1.8.0_282-b08
21/03/14 03:10:06 INFO org.sparkproject.jetty.server.Server: Started @5091ms
21/03/14 03:10:06 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@4eed6343{HTTP/1.1, (http/1.1)}{0.0.0.0:38829}
21/03/14 03:10:06 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-bc01-m/10.138.0.5:8032
21/03/14 03:10:07 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-bc01-m/10.138.0.5:10200
21/03/14 03:10:07 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
21/03/14 03:10:07 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
21/03/14 03:10:09 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1615691361088_0001
21/03/14 03:10:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-bc01-m/10.138.0.5:8030
21/03/14 03:10:13 INFO com.google.cloud.hadoop.io.bigquery.BigQueryFactory: BigQuery connector version hadoop2-1.2.0
21/03/14 03:10:13 INFO com.google.cloud.hadoop.io.bigquery.BigQueryFactory: Creating BigQuery from default credential.
21/03/14 03:10:13 INFO com.google.cloud.hadoop.io.bigquery.BigQueryFactory: Creating BigQuery from given credential.
21/03/14 03:10:13 INFO com.google.cloud.hadoop.io.bigquery.BigQueryConfiguration: Using working path: 'gs://dataproc-staging-us-west1-283579064512-mj5qi01m/hadoop/tmp/bigquerry/pyspark_input'
21/03/14 03:12:31 INFO com.google.cloud.hadoop.io.bigquery.UnshardedExportToCloudStorage: Setting FileInputFormat's inputPath to 'gs://dataproc-staging-us-west1-283579064512-mj5qi01m/hadoop/tmp/bigquerry/pyspark_input'
21/03/14 03:12:32 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 6
+-------+------------------+
|summary|     trip_distance|
+-------+------------------+
|  count|          24231126|
|   mean|3.5814057339315424|
| stddev| 327.8228554760617|
|    min|              0.01|
|    max|         350914.88|
+-------+------------------+

+-------+------------------+
|summary|      total_amount|
+-------+------------------+
|  count|          24231126|
|   mean|18.342393123230288|
| stddev|249.58870028047656|
|    min|               0.3|
|    max|          998325.6|
+-------+------------------+

21/03/14 03:18:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-staging-us-west1-283579064512-mj5qi01m/hadoop/tmp/bigquerry/' directory.
21/03/14 03:18:02 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@4eed6343{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
Job output is complete
```




